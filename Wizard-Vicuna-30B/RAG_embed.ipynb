{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ericy\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "Settings.llm = None\n",
    "Settings.chunk_size = 256\n",
    "Settings.chunk_overlap = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"Archive\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ericy\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 5\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=top_k,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.5)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How is attention retained?\"\n",
    "response = query_engine.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:\n",
      "Th  is chapter focuses on the mechanisms of selective att  ention and its role in perception and awareness.\n",
      "TAKE-HOME MESSAGES\n",
      " ■Selective attention is the ability to focus awareness on \n",
      "one stimulus, thought, or action while ignoring other, ir-relevant stimuli, thoughts, and actions.\n",
      " ■Arousal is a global physiological and psychological brain state, whereas selective attention describes what we attend and ignore within any specifi  c level (high versus low) of arousal.\n",
      " ■Specifi  c networks for the control of attention include cortical and subcortical structures.\n",
      " ■Attention infl  uences how we process sensory inputs, store that information in memory, process it semanti-cally, and act on it.\n",
      "\n",
      "1 Selective Attention and the Anatomy of Attention 276\n",
      "7 .2 The Neuropsychology of Attention \n",
      "278\n",
      "Neglect 278\n",
      "Comparing Neglect and Bálint’s \n",
      "Syndrome 281\n",
      "7 .3 Models of Attention 282\n",
      "Hermann von Helmholtz and Covert Attention \n",
      "282\n",
      "The Cocktail Party Effect 283\n",
      "Early-Selection Models Versus Late-Selection Models \n",
      "284\n",
      "Quantifying the Role of Attention in Perception \n",
      "285\n",
      "7 .4 Neural Mechanisms of Attention and Perceptual Selection \n",
      "287\n",
      "Voluntary Visuospatial Attention 287\n",
      "Reﬂ  exive Visuospatial Attention 296\n",
      "Visual Search 297\n",
      "Feature Attention 300\n",
      "Object Attention 304 Contents   |  xv\n",
      "\n",
      "Broadbent proposed such a model of attention.\n",
      " ■Late-selection models hypothesize that attended and ignored inputs are processed equivalently by the per-ceptual system, and that selection can occur only upon reaching a stage of semantic (meaning) encoding and analysis.must kee  p them fi  xed on a central fi  xation point (covert \n",
      "att ention), and the cue correctly predicts the target’s \n",
      "location, participants respond faster than when neutral \n",
      "cues are given (Figure 7.16). Th  is faster response dem-\n",
      "onstrates the benefi  ts of att  ention. In contrast, reaction \n",
      "times are longer (i.e., responses are slower) when the stimulus appears at an unexpected location, revealing the costs of att  ention.\n",
      "\n",
      "316  | CHAPTER 7   Attention\n",
      "may form a netw  ork that is critical for reorienting the \n",
      "focus of att  ention toward unexpected stimuli.\n",
      "To test this model, Corbett  a and his colleagues (2000) \n",
      "asked healthy participants to perform a task while their brain activity   was imaged with fMRI. Th  ey used the spa-tial cuing tasks described earlier (Figure 7.15), where cues predict the most likely location of a subsequent target but are sometimes incorrect, requiring participants to reorient att ention to process the target at the unexpected location. \n",
      "Stimuli that appeared in unexpected locations activated the TPJ; in fact, the right TPJ responded equally to novel stimuli in both the right and left   visual fi  elds.\n",
      "\n",
      "Oddly enough, he sees only the comb. “What about a spoon?” she asks.\n",
      "“Nope, no spoon,” he says, but then suddenly blurts out, “Y es, there it is, I see \n",
      "the spoon now.”\n",
      "“ Anything else?”Shaking his head, the patient replies, “Nope.”Shaking the spoon and the comb vigorously in front of her patient’s face, the \n",
      "doctor persists, “Y ou don’t see anything else, nothing at all?”\n",
      "He stares straight ahead, looking intently, and ﬁ  nally says, “Y es . . . yes, I see \n",
      "them now . . . I see some numbers.”\n",
      "“What?” says the puzzled doctor. “Numbers?”Attention\n",
      "BIG Questions\n",
      " ●Does attention affect perception?\n",
      " ●T o what extent does our conscious visual experience capture what we perceive?\n",
      " ●What neural mechanisms are involved in the control of attention?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context = \"Context:\\n\"\n",
    "for i in range(top_k):\n",
    "    context = context + response.source_nodes[i].text + \"\\n\\n\"\n",
    "\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "c:\\Users\\ericy\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\transformers\\modeling_utils.py:4713: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at ./Wizard-Vicuna-30B-Uncensored-GPTQ were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.bias', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.0.mlp.up_proj.bias', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.1.mlp.down_proj.bias', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.1.mlp.up_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.10.mlp.down_proj.bias', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.10.mlp.up_proj.bias', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.11.mlp.down_proj.bias', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.11.mlp.up_proj.bias', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.12.mlp.down_proj.bias', 'model.layers.12.mlp.gate_proj.bias', 'model.layers.12.mlp.up_proj.bias', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.13.mlp.down_proj.bias', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.13.mlp.up_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.14.mlp.down_proj.bias', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.14.mlp.up_proj.bias', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.15.mlp.down_proj.bias', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.15.mlp.up_proj.bias', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.16.mlp.down_proj.bias', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.16.mlp.up_proj.bias', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.17.mlp.down_proj.bias', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.17.mlp.up_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.18.mlp.down_proj.bias', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.18.mlp.up_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.19.mlp.down_proj.bias', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.19.mlp.up_proj.bias', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.2.mlp.down_proj.bias', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.2.mlp.up_proj.bias', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.20.mlp.down_proj.bias', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.20.mlp.up_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.21.mlp.down_proj.bias', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.21.mlp.up_proj.bias', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.22.mlp.down_proj.bias', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.22.mlp.up_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.23.mlp.down_proj.bias', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.23.mlp.up_proj.bias', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.24.mlp.down_proj.bias', 'model.layers.24.mlp.gate_proj.bias', 'model.layers.24.mlp.up_proj.bias', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.25.mlp.down_proj.bias', 'model.layers.25.mlp.gate_proj.bias', 'model.layers.25.mlp.up_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.25.self_attn.o_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.26.mlp.down_proj.bias', 'model.layers.26.mlp.gate_proj.bias', 'model.layers.26.mlp.up_proj.bias', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.27.mlp.down_proj.bias', 'model.layers.27.mlp.gate_proj.bias', 'model.layers.27.mlp.up_proj.bias', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.28.mlp.down_proj.bias', 'model.layers.28.mlp.gate_proj.bias', 'model.layers.28.mlp.up_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.28.self_attn.o_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.29.mlp.down_proj.bias', 'model.layers.29.mlp.gate_proj.bias', 'model.layers.29.mlp.up_proj.bias', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.29.self_attn.o_proj.bias', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.3.mlp.down_proj.bias', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.3.mlp.up_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.30.mlp.down_proj.bias', 'model.layers.30.mlp.gate_proj.bias', 'model.layers.30.mlp.up_proj.bias', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.30.self_attn.o_proj.bias', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.31.mlp.down_proj.bias', 'model.layers.31.mlp.gate_proj.bias', 'model.layers.31.mlp.up_proj.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.31.self_attn.o_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.32.mlp.down_proj.bias', 'model.layers.32.mlp.gate_proj.bias', 'model.layers.32.mlp.up_proj.bias', 'model.layers.32.self_attn.k_proj.bias', 'model.layers.32.self_attn.o_proj.bias', 'model.layers.32.self_attn.q_proj.bias', 'model.layers.32.self_attn.v_proj.bias', 'model.layers.33.mlp.down_proj.bias', 'model.layers.33.mlp.gate_proj.bias', 'model.layers.33.mlp.up_proj.bias', 'model.layers.33.self_attn.k_proj.bias', 'model.layers.33.self_attn.o_proj.bias', 'model.layers.33.self_attn.q_proj.bias', 'model.layers.33.self_attn.v_proj.bias', 'model.layers.34.mlp.down_proj.bias', 'model.layers.34.mlp.gate_proj.bias', 'model.layers.34.mlp.up_proj.bias', 'model.layers.34.self_attn.k_proj.bias', 'model.layers.34.self_attn.o_proj.bias', 'model.layers.34.self_attn.q_proj.bias', 'model.layers.34.self_attn.v_proj.bias', 'model.layers.35.mlp.down_proj.bias', 'model.layers.35.mlp.gate_proj.bias', 'model.layers.35.mlp.up_proj.bias', 'model.layers.35.self_attn.k_proj.bias', 'model.layers.35.self_attn.o_proj.bias', 'model.layers.35.self_attn.q_proj.bias', 'model.layers.35.self_attn.v_proj.bias', 'model.layers.36.mlp.down_proj.bias', 'model.layers.36.mlp.gate_proj.bias', 'model.layers.36.mlp.up_proj.bias', 'model.layers.36.self_attn.k_proj.bias', 'model.layers.36.self_attn.o_proj.bias', 'model.layers.36.self_attn.q_proj.bias', 'model.layers.36.self_attn.v_proj.bias', 'model.layers.37.mlp.down_proj.bias', 'model.layers.37.mlp.gate_proj.bias', 'model.layers.37.mlp.up_proj.bias', 'model.layers.37.self_attn.k_proj.bias', 'model.layers.37.self_attn.o_proj.bias', 'model.layers.37.self_attn.q_proj.bias', 'model.layers.37.self_attn.v_proj.bias', 'model.layers.38.mlp.down_proj.bias', 'model.layers.38.mlp.gate_proj.bias', 'model.layers.38.mlp.up_proj.bias', 'model.layers.38.self_attn.k_proj.bias', 'model.layers.38.self_attn.o_proj.bias', 'model.layers.38.self_attn.q_proj.bias', 'model.layers.38.self_attn.v_proj.bias', 'model.layers.39.mlp.down_proj.bias', 'model.layers.39.mlp.gate_proj.bias', 'model.layers.39.mlp.up_proj.bias', 'model.layers.39.self_attn.k_proj.bias', 'model.layers.39.self_attn.o_proj.bias', 'model.layers.39.self_attn.q_proj.bias', 'model.layers.39.self_attn.v_proj.bias', 'model.layers.4.mlp.down_proj.bias', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.4.mlp.up_proj.bias', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.40.mlp.down_proj.bias', 'model.layers.40.mlp.gate_proj.bias', 'model.layers.40.mlp.up_proj.bias', 'model.layers.40.self_attn.k_proj.bias', 'model.layers.40.self_attn.o_proj.bias', 'model.layers.40.self_attn.q_proj.bias', 'model.layers.40.self_attn.v_proj.bias', 'model.layers.41.mlp.down_proj.bias', 'model.layers.41.mlp.gate_proj.bias', 'model.layers.41.mlp.up_proj.bias', 'model.layers.41.self_attn.k_proj.bias', 'model.layers.41.self_attn.o_proj.bias', 'model.layers.41.self_attn.q_proj.bias', 'model.layers.41.self_attn.v_proj.bias', 'model.layers.42.mlp.down_proj.bias', 'model.layers.42.mlp.gate_proj.bias', 'model.layers.42.mlp.up_proj.bias', 'model.layers.42.self_attn.k_proj.bias', 'model.layers.42.self_attn.o_proj.bias', 'model.layers.42.self_attn.q_proj.bias', 'model.layers.42.self_attn.v_proj.bias', 'model.layers.43.mlp.down_proj.bias', 'model.layers.43.mlp.gate_proj.bias', 'model.layers.43.mlp.up_proj.bias', 'model.layers.43.self_attn.k_proj.bias', 'model.layers.43.self_attn.o_proj.bias', 'model.layers.43.self_attn.q_proj.bias', 'model.layers.43.self_attn.v_proj.bias', 'model.layers.44.mlp.down_proj.bias', 'model.layers.44.mlp.gate_proj.bias', 'model.layers.44.mlp.up_proj.bias', 'model.layers.44.self_attn.k_proj.bias', 'model.layers.44.self_attn.o_proj.bias', 'model.layers.44.self_attn.q_proj.bias', 'model.layers.44.self_attn.v_proj.bias', 'model.layers.45.mlp.down_proj.bias', 'model.layers.45.mlp.gate_proj.bias', 'model.layers.45.mlp.up_proj.bias', 'model.layers.45.self_attn.k_proj.bias', 'model.layers.45.self_attn.o_proj.bias', 'model.layers.45.self_attn.q_proj.bias', 'model.layers.45.self_attn.v_proj.bias', 'model.layers.46.mlp.down_proj.bias', 'model.layers.46.mlp.gate_proj.bias', 'model.layers.46.mlp.up_proj.bias', 'model.layers.46.self_attn.k_proj.bias', 'model.layers.46.self_attn.o_proj.bias', 'model.layers.46.self_attn.q_proj.bias', 'model.layers.46.self_attn.v_proj.bias', 'model.layers.47.mlp.down_proj.bias', 'model.layers.47.mlp.gate_proj.bias', 'model.layers.47.mlp.up_proj.bias', 'model.layers.47.self_attn.k_proj.bias', 'model.layers.47.self_attn.o_proj.bias', 'model.layers.47.self_attn.q_proj.bias', 'model.layers.47.self_attn.v_proj.bias', 'model.layers.48.mlp.down_proj.bias', 'model.layers.48.mlp.gate_proj.bias', 'model.layers.48.mlp.up_proj.bias', 'model.layers.48.self_attn.k_proj.bias', 'model.layers.48.self_attn.o_proj.bias', 'model.layers.48.self_attn.q_proj.bias', 'model.layers.48.self_attn.v_proj.bias', 'model.layers.49.mlp.down_proj.bias', 'model.layers.49.mlp.gate_proj.bias', 'model.layers.49.mlp.up_proj.bias', 'model.layers.49.self_attn.k_proj.bias', 'model.layers.49.self_attn.o_proj.bias', 'model.layers.49.self_attn.q_proj.bias', 'model.layers.49.self_attn.v_proj.bias', 'model.layers.5.mlp.down_proj.bias', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.5.mlp.up_proj.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.50.mlp.down_proj.bias', 'model.layers.50.mlp.gate_proj.bias', 'model.layers.50.mlp.up_proj.bias', 'model.layers.50.self_attn.k_proj.bias', 'model.layers.50.self_attn.o_proj.bias', 'model.layers.50.self_attn.q_proj.bias', 'model.layers.50.self_attn.v_proj.bias', 'model.layers.51.mlp.down_proj.bias', 'model.layers.51.mlp.gate_proj.bias', 'model.layers.51.mlp.up_proj.bias', 'model.layers.51.self_attn.k_proj.bias', 'model.layers.51.self_attn.o_proj.bias', 'model.layers.51.self_attn.q_proj.bias', 'model.layers.51.self_attn.v_proj.bias', 'model.layers.52.mlp.down_proj.bias', 'model.layers.52.mlp.gate_proj.bias', 'model.layers.52.mlp.up_proj.bias', 'model.layers.52.self_attn.k_proj.bias', 'model.layers.52.self_attn.o_proj.bias', 'model.layers.52.self_attn.q_proj.bias', 'model.layers.52.self_attn.v_proj.bias', 'model.layers.53.mlp.down_proj.bias', 'model.layers.53.mlp.gate_proj.bias', 'model.layers.53.mlp.up_proj.bias', 'model.layers.53.self_attn.k_proj.bias', 'model.layers.53.self_attn.o_proj.bias', 'model.layers.53.self_attn.q_proj.bias', 'model.layers.53.self_attn.v_proj.bias', 'model.layers.54.mlp.down_proj.bias', 'model.layers.54.mlp.gate_proj.bias', 'model.layers.54.mlp.up_proj.bias', 'model.layers.54.self_attn.k_proj.bias', 'model.layers.54.self_attn.o_proj.bias', 'model.layers.54.self_attn.q_proj.bias', 'model.layers.54.self_attn.v_proj.bias', 'model.layers.55.mlp.down_proj.bias', 'model.layers.55.mlp.gate_proj.bias', 'model.layers.55.mlp.up_proj.bias', 'model.layers.55.self_attn.k_proj.bias', 'model.layers.55.self_attn.o_proj.bias', 'model.layers.55.self_attn.q_proj.bias', 'model.layers.55.self_attn.v_proj.bias', 'model.layers.56.mlp.down_proj.bias', 'model.layers.56.mlp.gate_proj.bias', 'model.layers.56.mlp.up_proj.bias', 'model.layers.56.self_attn.k_proj.bias', 'model.layers.56.self_attn.o_proj.bias', 'model.layers.56.self_attn.q_proj.bias', 'model.layers.56.self_attn.v_proj.bias', 'model.layers.57.mlp.down_proj.bias', 'model.layers.57.mlp.gate_proj.bias', 'model.layers.57.mlp.up_proj.bias', 'model.layers.57.self_attn.k_proj.bias', 'model.layers.57.self_attn.o_proj.bias', 'model.layers.57.self_attn.q_proj.bias', 'model.layers.57.self_attn.v_proj.bias', 'model.layers.58.mlp.down_proj.bias', 'model.layers.58.mlp.gate_proj.bias', 'model.layers.58.mlp.up_proj.bias', 'model.layers.58.self_attn.k_proj.bias', 'model.layers.58.self_attn.o_proj.bias', 'model.layers.58.self_attn.q_proj.bias', 'model.layers.58.self_attn.v_proj.bias', 'model.layers.59.mlp.down_proj.bias', 'model.layers.59.mlp.gate_proj.bias', 'model.layers.59.mlp.up_proj.bias', 'model.layers.59.self_attn.k_proj.bias', 'model.layers.59.self_attn.o_proj.bias', 'model.layers.59.self_attn.q_proj.bias', 'model.layers.59.self_attn.v_proj.bias', 'model.layers.6.mlp.down_proj.bias', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.6.mlp.up_proj.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.7.mlp.down_proj.bias', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.7.mlp.up_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.8.mlp.down_proj.bias', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.8.mlp.up_proj.bias', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.9.mlp.down_proj.bias', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.9.mlp.up_proj.bias', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.9.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at ./Wizard-Vicuna-30B-Uncensored-GPTQ and are newly initialized: ['model.layers.0.mlp.down_proj.g_idx', 'model.layers.0.mlp.gate_proj.g_idx', 'model.layers.0.mlp.up_proj.g_idx', 'model.layers.0.self_attn.k_proj.g_idx', 'model.layers.0.self_attn.o_proj.g_idx', 'model.layers.0.self_attn.q_proj.g_idx', 'model.layers.0.self_attn.v_proj.g_idx', 'model.layers.1.mlp.down_proj.g_idx', 'model.layers.1.mlp.gate_proj.g_idx', 'model.layers.1.mlp.up_proj.g_idx', 'model.layers.1.self_attn.k_proj.g_idx', 'model.layers.1.self_attn.o_proj.g_idx', 'model.layers.1.self_attn.q_proj.g_idx', 'model.layers.1.self_attn.v_proj.g_idx', 'model.layers.10.mlp.down_proj.g_idx', 'model.layers.10.mlp.gate_proj.g_idx', 'model.layers.10.mlp.up_proj.g_idx', 'model.layers.10.self_attn.k_proj.g_idx', 'model.layers.10.self_attn.o_proj.g_idx', 'model.layers.10.self_attn.q_proj.g_idx', 'model.layers.10.self_attn.v_proj.g_idx', 'model.layers.11.mlp.down_proj.g_idx', 'model.layers.11.mlp.gate_proj.g_idx', 'model.layers.11.mlp.up_proj.g_idx', 'model.layers.11.self_attn.k_proj.g_idx', 'model.layers.11.self_attn.o_proj.g_idx', 'model.layers.11.self_attn.q_proj.g_idx', 'model.layers.11.self_attn.v_proj.g_idx', 'model.layers.12.mlp.down_proj.g_idx', 'model.layers.12.mlp.gate_proj.g_idx', 'model.layers.12.mlp.up_proj.g_idx', 'model.layers.12.self_attn.k_proj.g_idx', 'model.layers.12.self_attn.o_proj.g_idx', 'model.layers.12.self_attn.q_proj.g_idx', 'model.layers.12.self_attn.v_proj.g_idx', 'model.layers.13.mlp.down_proj.g_idx', 'model.layers.13.mlp.gate_proj.g_idx', 'model.layers.13.mlp.up_proj.g_idx', 'model.layers.13.self_attn.k_proj.g_idx', 'model.layers.13.self_attn.o_proj.g_idx', 'model.layers.13.self_attn.q_proj.g_idx', 'model.layers.13.self_attn.v_proj.g_idx', 'model.layers.14.mlp.down_proj.g_idx', 'model.layers.14.mlp.gate_proj.g_idx', 'model.layers.14.mlp.up_proj.g_idx', 'model.layers.14.self_attn.k_proj.g_idx', 'model.layers.14.self_attn.o_proj.g_idx', 'model.layers.14.self_attn.q_proj.g_idx', 'model.layers.14.self_attn.v_proj.g_idx', 'model.layers.15.mlp.down_proj.g_idx', 'model.layers.15.mlp.gate_proj.g_idx', 'model.layers.15.mlp.up_proj.g_idx', 'model.layers.15.self_attn.k_proj.g_idx', 'model.layers.15.self_attn.o_proj.g_idx', 'model.layers.15.self_attn.q_proj.g_idx', 'model.layers.15.self_attn.v_proj.g_idx', 'model.layers.16.mlp.down_proj.g_idx', 'model.layers.16.mlp.gate_proj.g_idx', 'model.layers.16.mlp.up_proj.g_idx', 'model.layers.16.self_attn.k_proj.g_idx', 'model.layers.16.self_attn.o_proj.g_idx', 'model.layers.16.self_attn.q_proj.g_idx', 'model.layers.16.self_attn.v_proj.g_idx', 'model.layers.17.mlp.down_proj.g_idx', 'model.layers.17.mlp.gate_proj.g_idx', 'model.layers.17.mlp.up_proj.g_idx', 'model.layers.17.self_attn.k_proj.g_idx', 'model.layers.17.self_attn.o_proj.g_idx', 'model.layers.17.self_attn.q_proj.g_idx', 'model.layers.17.self_attn.v_proj.g_idx', 'model.layers.18.mlp.down_proj.g_idx', 'model.layers.18.mlp.gate_proj.g_idx', 'model.layers.18.mlp.up_proj.g_idx', 'model.layers.18.self_attn.k_proj.g_idx', 'model.layers.18.self_attn.o_proj.g_idx', 'model.layers.18.self_attn.q_proj.g_idx', 'model.layers.18.self_attn.v_proj.g_idx', 'model.layers.19.mlp.down_proj.g_idx', 'model.layers.19.mlp.gate_proj.g_idx', 'model.layers.19.mlp.up_proj.g_idx', 'model.layers.19.self_attn.k_proj.g_idx', 'model.layers.19.self_attn.o_proj.g_idx', 'model.layers.19.self_attn.q_proj.g_idx', 'model.layers.19.self_attn.v_proj.g_idx', 'model.layers.2.mlp.down_proj.g_idx', 'model.layers.2.mlp.gate_proj.g_idx', 'model.layers.2.mlp.up_proj.g_idx', 'model.layers.2.self_attn.k_proj.g_idx', 'model.layers.2.self_attn.o_proj.g_idx', 'model.layers.2.self_attn.q_proj.g_idx', 'model.layers.2.self_attn.v_proj.g_idx', 'model.layers.20.mlp.down_proj.g_idx', 'model.layers.20.mlp.gate_proj.g_idx', 'model.layers.20.mlp.up_proj.g_idx', 'model.layers.20.self_attn.k_proj.g_idx', 'model.layers.20.self_attn.o_proj.g_idx', 'model.layers.20.self_attn.q_proj.g_idx', 'model.layers.20.self_attn.v_proj.g_idx', 'model.layers.21.mlp.down_proj.g_idx', 'model.layers.21.mlp.gate_proj.g_idx', 'model.layers.21.mlp.up_proj.g_idx', 'model.layers.21.self_attn.k_proj.g_idx', 'model.layers.21.self_attn.o_proj.g_idx', 'model.layers.21.self_attn.q_proj.g_idx', 'model.layers.21.self_attn.v_proj.g_idx', 'model.layers.22.mlp.down_proj.g_idx', 'model.layers.22.mlp.gate_proj.g_idx', 'model.layers.22.mlp.up_proj.g_idx', 'model.layers.22.self_attn.k_proj.g_idx', 'model.layers.22.self_attn.o_proj.g_idx', 'model.layers.22.self_attn.q_proj.g_idx', 'model.layers.22.self_attn.v_proj.g_idx', 'model.layers.23.mlp.down_proj.g_idx', 'model.layers.23.mlp.gate_proj.g_idx', 'model.layers.23.mlp.up_proj.g_idx', 'model.layers.23.self_attn.k_proj.g_idx', 'model.layers.23.self_attn.o_proj.g_idx', 'model.layers.23.self_attn.q_proj.g_idx', 'model.layers.23.self_attn.v_proj.g_idx', 'model.layers.24.mlp.down_proj.g_idx', 'model.layers.24.mlp.gate_proj.g_idx', 'model.layers.24.mlp.up_proj.g_idx', 'model.layers.24.self_attn.k_proj.g_idx', 'model.layers.24.self_attn.o_proj.g_idx', 'model.layers.24.self_attn.q_proj.g_idx', 'model.layers.24.self_attn.v_proj.g_idx', 'model.layers.25.mlp.down_proj.g_idx', 'model.layers.25.mlp.gate_proj.g_idx', 'model.layers.25.mlp.up_proj.g_idx', 'model.layers.25.self_attn.k_proj.g_idx', 'model.layers.25.self_attn.o_proj.g_idx', 'model.layers.25.self_attn.q_proj.g_idx', 'model.layers.25.self_attn.v_proj.g_idx', 'model.layers.26.mlp.down_proj.g_idx', 'model.layers.26.mlp.gate_proj.g_idx', 'model.layers.26.mlp.up_proj.g_idx', 'model.layers.26.self_attn.k_proj.g_idx', 'model.layers.26.self_attn.o_proj.g_idx', 'model.layers.26.self_attn.q_proj.g_idx', 'model.layers.26.self_attn.v_proj.g_idx', 'model.layers.27.mlp.down_proj.g_idx', 'model.layers.27.mlp.gate_proj.g_idx', 'model.layers.27.mlp.up_proj.g_idx', 'model.layers.27.self_attn.k_proj.g_idx', 'model.layers.27.self_attn.o_proj.g_idx', 'model.layers.27.self_attn.q_proj.g_idx', 'model.layers.27.self_attn.v_proj.g_idx', 'model.layers.28.mlp.down_proj.g_idx', 'model.layers.28.mlp.gate_proj.g_idx', 'model.layers.28.mlp.up_proj.g_idx', 'model.layers.28.self_attn.k_proj.g_idx', 'model.layers.28.self_attn.o_proj.g_idx', 'model.layers.28.self_attn.q_proj.g_idx', 'model.layers.28.self_attn.v_proj.g_idx', 'model.layers.29.mlp.down_proj.g_idx', 'model.layers.29.mlp.gate_proj.g_idx', 'model.layers.29.mlp.up_proj.g_idx', 'model.layers.29.self_attn.k_proj.g_idx', 'model.layers.29.self_attn.o_proj.g_idx', 'model.layers.29.self_attn.q_proj.g_idx', 'model.layers.29.self_attn.v_proj.g_idx', 'model.layers.3.mlp.down_proj.g_idx', 'model.layers.3.mlp.gate_proj.g_idx', 'model.layers.3.mlp.up_proj.g_idx', 'model.layers.3.self_attn.k_proj.g_idx', 'model.layers.3.self_attn.o_proj.g_idx', 'model.layers.3.self_attn.q_proj.g_idx', 'model.layers.3.self_attn.v_proj.g_idx', 'model.layers.30.mlp.down_proj.g_idx', 'model.layers.30.mlp.gate_proj.g_idx', 'model.layers.30.mlp.up_proj.g_idx', 'model.layers.30.self_attn.k_proj.g_idx', 'model.layers.30.self_attn.o_proj.g_idx', 'model.layers.30.self_attn.q_proj.g_idx', 'model.layers.30.self_attn.v_proj.g_idx', 'model.layers.31.mlp.down_proj.g_idx', 'model.layers.31.mlp.gate_proj.g_idx', 'model.layers.31.mlp.up_proj.g_idx', 'model.layers.31.self_attn.k_proj.g_idx', 'model.layers.31.self_attn.o_proj.g_idx', 'model.layers.31.self_attn.q_proj.g_idx', 'model.layers.31.self_attn.v_proj.g_idx', 'model.layers.32.mlp.down_proj.g_idx', 'model.layers.32.mlp.gate_proj.g_idx', 'model.layers.32.mlp.up_proj.g_idx', 'model.layers.32.self_attn.k_proj.g_idx', 'model.layers.32.self_attn.o_proj.g_idx', 'model.layers.32.self_attn.q_proj.g_idx', 'model.layers.32.self_attn.v_proj.g_idx', 'model.layers.33.mlp.down_proj.g_idx', 'model.layers.33.mlp.gate_proj.g_idx', 'model.layers.33.mlp.up_proj.g_idx', 'model.layers.33.self_attn.k_proj.g_idx', 'model.layers.33.self_attn.o_proj.g_idx', 'model.layers.33.self_attn.q_proj.g_idx', 'model.layers.33.self_attn.v_proj.g_idx', 'model.layers.34.mlp.down_proj.g_idx', 'model.layers.34.mlp.gate_proj.g_idx', 'model.layers.34.mlp.up_proj.g_idx', 'model.layers.34.self_attn.k_proj.g_idx', 'model.layers.34.self_attn.o_proj.g_idx', 'model.layers.34.self_attn.q_proj.g_idx', 'model.layers.34.self_attn.v_proj.g_idx', 'model.layers.35.mlp.down_proj.g_idx', 'model.layers.35.mlp.gate_proj.g_idx', 'model.layers.35.mlp.up_proj.g_idx', 'model.layers.35.self_attn.k_proj.g_idx', 'model.layers.35.self_attn.o_proj.g_idx', 'model.layers.35.self_attn.q_proj.g_idx', 'model.layers.35.self_attn.v_proj.g_idx', 'model.layers.36.mlp.down_proj.g_idx', 'model.layers.36.mlp.gate_proj.g_idx', 'model.layers.36.mlp.up_proj.g_idx', 'model.layers.36.self_attn.k_proj.g_idx', 'model.layers.36.self_attn.o_proj.g_idx', 'model.layers.36.self_attn.q_proj.g_idx', 'model.layers.36.self_attn.v_proj.g_idx', 'model.layers.37.mlp.down_proj.g_idx', 'model.layers.37.mlp.gate_proj.g_idx', 'model.layers.37.mlp.up_proj.g_idx', 'model.layers.37.self_attn.k_proj.g_idx', 'model.layers.37.self_attn.o_proj.g_idx', 'model.layers.37.self_attn.q_proj.g_idx', 'model.layers.37.self_attn.v_proj.g_idx', 'model.layers.38.mlp.down_proj.g_idx', 'model.layers.38.mlp.gate_proj.g_idx', 'model.layers.38.mlp.up_proj.g_idx', 'model.layers.38.self_attn.k_proj.g_idx', 'model.layers.38.self_attn.o_proj.g_idx', 'model.layers.38.self_attn.q_proj.g_idx', 'model.layers.38.self_attn.v_proj.g_idx', 'model.layers.39.mlp.down_proj.g_idx', 'model.layers.39.mlp.gate_proj.g_idx', 'model.layers.39.mlp.up_proj.g_idx', 'model.layers.39.self_attn.k_proj.g_idx', 'model.layers.39.self_attn.o_proj.g_idx', 'model.layers.39.self_attn.q_proj.g_idx', 'model.layers.39.self_attn.v_proj.g_idx', 'model.layers.4.mlp.down_proj.g_idx', 'model.layers.4.mlp.gate_proj.g_idx', 'model.layers.4.mlp.up_proj.g_idx', 'model.layers.4.self_attn.k_proj.g_idx', 'model.layers.4.self_attn.o_proj.g_idx', 'model.layers.4.self_attn.q_proj.g_idx', 'model.layers.4.self_attn.v_proj.g_idx', 'model.layers.40.mlp.down_proj.g_idx', 'model.layers.40.mlp.gate_proj.g_idx', 'model.layers.40.mlp.up_proj.g_idx', 'model.layers.40.self_attn.k_proj.g_idx', 'model.layers.40.self_attn.o_proj.g_idx', 'model.layers.40.self_attn.q_proj.g_idx', 'model.layers.40.self_attn.v_proj.g_idx', 'model.layers.41.mlp.down_proj.g_idx', 'model.layers.41.mlp.gate_proj.g_idx', 'model.layers.41.mlp.up_proj.g_idx', 'model.layers.41.self_attn.k_proj.g_idx', 'model.layers.41.self_attn.o_proj.g_idx', 'model.layers.41.self_attn.q_proj.g_idx', 'model.layers.41.self_attn.v_proj.g_idx', 'model.layers.42.mlp.down_proj.g_idx', 'model.layers.42.mlp.gate_proj.g_idx', 'model.layers.42.mlp.up_proj.g_idx', 'model.layers.42.self_attn.k_proj.g_idx', 'model.layers.42.self_attn.o_proj.g_idx', 'model.layers.42.self_attn.q_proj.g_idx', 'model.layers.42.self_attn.v_proj.g_idx', 'model.layers.43.mlp.down_proj.g_idx', 'model.layers.43.mlp.gate_proj.g_idx', 'model.layers.43.mlp.up_proj.g_idx', 'model.layers.43.self_attn.k_proj.g_idx', 'model.layers.43.self_attn.o_proj.g_idx', 'model.layers.43.self_attn.q_proj.g_idx', 'model.layers.43.self_attn.v_proj.g_idx', 'model.layers.44.mlp.down_proj.g_idx', 'model.layers.44.mlp.gate_proj.g_idx', 'model.layers.44.mlp.up_proj.g_idx', 'model.layers.44.self_attn.k_proj.g_idx', 'model.layers.44.self_attn.o_proj.g_idx', 'model.layers.44.self_attn.q_proj.g_idx', 'model.layers.44.self_attn.v_proj.g_idx', 'model.layers.45.mlp.down_proj.g_idx', 'model.layers.45.mlp.gate_proj.g_idx', 'model.layers.45.mlp.up_proj.g_idx', 'model.layers.45.self_attn.k_proj.g_idx', 'model.layers.45.self_attn.o_proj.g_idx', 'model.layers.45.self_attn.q_proj.g_idx', 'model.layers.45.self_attn.v_proj.g_idx', 'model.layers.46.mlp.down_proj.g_idx', 'model.layers.46.mlp.gate_proj.g_idx', 'model.layers.46.mlp.up_proj.g_idx', 'model.layers.46.self_attn.k_proj.g_idx', 'model.layers.46.self_attn.o_proj.g_idx', 'model.layers.46.self_attn.q_proj.g_idx', 'model.layers.46.self_attn.v_proj.g_idx', 'model.layers.47.mlp.down_proj.g_idx', 'model.layers.47.mlp.gate_proj.g_idx', 'model.layers.47.mlp.up_proj.g_idx', 'model.layers.47.self_attn.k_proj.g_idx', 'model.layers.47.self_attn.o_proj.g_idx', 'model.layers.47.self_attn.q_proj.g_idx', 'model.layers.47.self_attn.v_proj.g_idx', 'model.layers.48.mlp.down_proj.g_idx', 'model.layers.48.mlp.gate_proj.g_idx', 'model.layers.48.mlp.up_proj.g_idx', 'model.layers.48.self_attn.k_proj.g_idx', 'model.layers.48.self_attn.o_proj.g_idx', 'model.layers.48.self_attn.q_proj.g_idx', 'model.layers.48.self_attn.v_proj.g_idx', 'model.layers.49.mlp.down_proj.g_idx', 'model.layers.49.mlp.gate_proj.g_idx', 'model.layers.49.mlp.up_proj.g_idx', 'model.layers.49.self_attn.k_proj.g_idx', 'model.layers.49.self_attn.o_proj.g_idx', 'model.layers.49.self_attn.q_proj.g_idx', 'model.layers.49.self_attn.v_proj.g_idx', 'model.layers.5.mlp.down_proj.g_idx', 'model.layers.5.mlp.gate_proj.g_idx', 'model.layers.5.mlp.up_proj.g_idx', 'model.layers.5.self_attn.k_proj.g_idx', 'model.layers.5.self_attn.o_proj.g_idx', 'model.layers.5.self_attn.q_proj.g_idx', 'model.layers.5.self_attn.v_proj.g_idx', 'model.layers.50.mlp.down_proj.g_idx', 'model.layers.50.mlp.gate_proj.g_idx', 'model.layers.50.mlp.up_proj.g_idx', 'model.layers.50.self_attn.k_proj.g_idx', 'model.layers.50.self_attn.o_proj.g_idx', 'model.layers.50.self_attn.q_proj.g_idx', 'model.layers.50.self_attn.v_proj.g_idx', 'model.layers.51.mlp.down_proj.g_idx', 'model.layers.51.mlp.gate_proj.g_idx', 'model.layers.51.mlp.up_proj.g_idx', 'model.layers.51.self_attn.k_proj.g_idx', 'model.layers.51.self_attn.o_proj.g_idx', 'model.layers.51.self_attn.q_proj.g_idx', 'model.layers.51.self_attn.v_proj.g_idx', 'model.layers.52.mlp.down_proj.g_idx', 'model.layers.52.mlp.gate_proj.g_idx', 'model.layers.52.mlp.up_proj.g_idx', 'model.layers.52.self_attn.k_proj.g_idx', 'model.layers.52.self_attn.o_proj.g_idx', 'model.layers.52.self_attn.q_proj.g_idx', 'model.layers.52.self_attn.v_proj.g_idx', 'model.layers.53.mlp.down_proj.g_idx', 'model.layers.53.mlp.gate_proj.g_idx', 'model.layers.53.mlp.up_proj.g_idx', 'model.layers.53.self_attn.k_proj.g_idx', 'model.layers.53.self_attn.o_proj.g_idx', 'model.layers.53.self_attn.q_proj.g_idx', 'model.layers.53.self_attn.v_proj.g_idx', 'model.layers.54.mlp.down_proj.g_idx', 'model.layers.54.mlp.gate_proj.g_idx', 'model.layers.54.mlp.up_proj.g_idx', 'model.layers.54.self_attn.k_proj.g_idx', 'model.layers.54.self_attn.o_proj.g_idx', 'model.layers.54.self_attn.q_proj.g_idx', 'model.layers.54.self_attn.v_proj.g_idx', 'model.layers.55.mlp.down_proj.g_idx', 'model.layers.55.mlp.gate_proj.g_idx', 'model.layers.55.mlp.up_proj.g_idx', 'model.layers.55.self_attn.k_proj.g_idx', 'model.layers.55.self_attn.o_proj.g_idx', 'model.layers.55.self_attn.q_proj.g_idx', 'model.layers.55.self_attn.v_proj.g_idx', 'model.layers.56.mlp.down_proj.g_idx', 'model.layers.56.mlp.gate_proj.g_idx', 'model.layers.56.mlp.up_proj.g_idx', 'model.layers.56.self_attn.k_proj.g_idx', 'model.layers.56.self_attn.o_proj.g_idx', 'model.layers.56.self_attn.q_proj.g_idx', 'model.layers.56.self_attn.v_proj.g_idx', 'model.layers.57.mlp.down_proj.g_idx', 'model.layers.57.mlp.gate_proj.g_idx', 'model.layers.57.mlp.up_proj.g_idx', 'model.layers.57.self_attn.k_proj.g_idx', 'model.layers.57.self_attn.o_proj.g_idx', 'model.layers.57.self_attn.q_proj.g_idx', 'model.layers.57.self_attn.v_proj.g_idx', 'model.layers.58.mlp.down_proj.g_idx', 'model.layers.58.mlp.gate_proj.g_idx', 'model.layers.58.mlp.up_proj.g_idx', 'model.layers.58.self_attn.k_proj.g_idx', 'model.layers.58.self_attn.o_proj.g_idx', 'model.layers.58.self_attn.q_proj.g_idx', 'model.layers.58.self_attn.v_proj.g_idx', 'model.layers.59.mlp.down_proj.g_idx', 'model.layers.59.mlp.gate_proj.g_idx', 'model.layers.59.mlp.up_proj.g_idx', 'model.layers.59.self_attn.k_proj.g_idx', 'model.layers.59.self_attn.o_proj.g_idx', 'model.layers.59.self_attn.q_proj.g_idx', 'model.layers.59.self_attn.v_proj.g_idx', 'model.layers.6.mlp.down_proj.g_idx', 'model.layers.6.mlp.gate_proj.g_idx', 'model.layers.6.mlp.up_proj.g_idx', 'model.layers.6.self_attn.k_proj.g_idx', 'model.layers.6.self_attn.o_proj.g_idx', 'model.layers.6.self_attn.q_proj.g_idx', 'model.layers.6.self_attn.v_proj.g_idx', 'model.layers.7.mlp.down_proj.g_idx', 'model.layers.7.mlp.gate_proj.g_idx', 'model.layers.7.mlp.up_proj.g_idx', 'model.layers.7.self_attn.k_proj.g_idx', 'model.layers.7.self_attn.o_proj.g_idx', 'model.layers.7.self_attn.q_proj.g_idx', 'model.layers.7.self_attn.v_proj.g_idx', 'model.layers.8.mlp.down_proj.g_idx', 'model.layers.8.mlp.gate_proj.g_idx', 'model.layers.8.mlp.up_proj.g_idx', 'model.layers.8.self_attn.k_proj.g_idx', 'model.layers.8.self_attn.o_proj.g_idx', 'model.layers.8.self_attn.q_proj.g_idx', 'model.layers.8.self_attn.v_proj.g_idx', 'model.layers.9.mlp.down_proj.g_idx', 'model.layers.9.mlp.gate_proj.g_idx', 'model.layers.9.mlp.up_proj.g_idx', 'model.layers.9.self_attn.k_proj.g_idx', 'model.layers.9.self_attn.o_proj.g_idx', 'model.layers.9.self_attn.q_proj.g_idx', 'model.layers.9.self_attn.v_proj.g_idx']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     37\u001b[0m     user \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSER: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 38\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[43mget_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     40\u001b[0m     input_text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m USER:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m context \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m user \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ASSISTANT: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[14], line 27\u001b[0m, in \u001b[0;36mget_query\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_query\u001b[39m(query):\n\u001b[1;32m---> 27\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mquery_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContext:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(top_k):\n",
      "File \u001b[1;32mc:\\Users\\ericy\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:307\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    304\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 307\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[0;32m    309\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[0;32m    310\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n",
      "File \u001b[1;32mc:\\Users\\ericy\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\llama_index\\core\\base\\base_query_engine.py:52\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[1;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     51\u001b[0m         str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[1;32m---> 52\u001b[0m     query_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_or_query_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m dispatcher\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m     54\u001b[0m     QueryEndEvent(query\u001b[38;5;241m=\u001b[39mstr_or_query_bundle, response\u001b[38;5;241m=\u001b[39mquery_result)\n\u001b[0;32m     55\u001b[0m )\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m query_result\n",
      "File \u001b[1;32mc:\\Users\\ericy\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:307\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    304\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 307\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[0;32m    309\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[0;32m    310\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n",
      "File \u001b[1;32mc:\\Users\\ericy\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\llama_index\\core\\query_engine\\retriever_query_engine.py:177\u001b[0m, in \u001b[0;36mRetrieverQueryEngine._query\u001b[1;34m(self, query_bundle)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Answer a query.\"\"\"\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    175\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mQUERY, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_bundle\u001b[38;5;241m.\u001b[39mquery_str}\n\u001b[0;32m    176\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m query_event:\n\u001b[1;32m--> 177\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_synthesizer\u001b[38;5;241m.\u001b[39msynthesize(\n\u001b[0;32m    179\u001b[0m         query\u001b[38;5;241m=\u001b[39mquery_bundle,\n\u001b[0;32m    180\u001b[0m         nodes\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[0;32m    181\u001b[0m     )\n\u001b[0;32m    182\u001b[0m     query_event\u001b[38;5;241m.\u001b[39mon_end(payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mRESPONSE: response})\n",
      "File \u001b[1;32mc:\\Users\\ericy\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\llama_index\\core\\query_engine\\retriever_query_engine.py:132\u001b[0m, in \u001b[0;36mRetrieverQueryEngine.retrieve\u001b[1;34m(self, query_bundle)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieve\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_bundle: QueryBundle) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[NodeWithScore]:\n\u001b[1;32m--> 132\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_node_postprocessors(nodes, query_bundle\u001b[38;5;241m=\u001b[39mquery_bundle)\n",
      "File \u001b[1;32mc:\\Users\\ericy\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:307\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    304\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 307\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[0;32m    309\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[0;32m    310\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n",
      "File \u001b[1;32mc:\\Users\\ericy\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\llama_index\\core\\base\\base_retriever.py:245\u001b[0m, in \u001b[0;36mBaseRetriever.retrieve\u001b[1;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mas_trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    242\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mRETRIEVE,\n\u001b[0;32m    243\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_bundle\u001b[38;5;241m.\u001b[39mquery_str},\n\u001b[0;32m    244\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m retrieve_event:\n\u001b[1;32m--> 245\u001b[0m         nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m         nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_recursive_retrieval(query_bundle, nodes)\n\u001b[0;32m    247\u001b[0m         retrieve_event\u001b[38;5;241m.\u001b[39mon_end(\n\u001b[0;32m    248\u001b[0m             payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mNODES: nodes},\n\u001b[0;32m    249\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\ericy\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:307\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    304\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 307\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[0;32m    309\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[0;32m    310\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n",
      "File \u001b[1;32mc:\\Users\\ericy\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\llama_index\\core\\indices\\vector_store\\retrievers\\retriever.py:103\u001b[0m, in \u001b[0;36mVectorIndexRetriever._retrieve\u001b[1;34m(self, query_bundle)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query_bundle\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(query_bundle\u001b[38;5;241m.\u001b[39membedding_strs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     98\u001b[0m         query_bundle\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     99\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model\u001b[38;5;241m.\u001b[39mget_agg_embedding_from_queries(\n\u001b[0;32m    100\u001b[0m                 query_bundle\u001b[38;5;241m.\u001b[39membedding_strs\n\u001b[0;32m    101\u001b[0m             )\n\u001b[0;32m    102\u001b[0m         )\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_nodes_with_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ericy\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\llama_index\\core\\indices\\vector_store\\retrievers\\retriever.py:180\u001b[0m, in \u001b[0;36mVectorIndexRetriever._get_nodes_with_embeddings\u001b[1;34m(self, query_bundle_with_embeddings)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_nodes_with_embeddings\u001b[39m(\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28mself\u001b[39m, query_bundle_with_embeddings: QueryBundle\n\u001b[0;32m    178\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[NodeWithScore]:\n\u001b[0;32m    179\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_vector_store_query(query_bundle_with_embeddings)\n\u001b[1;32m--> 180\u001b[0m     query_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vector_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_node_list_from_query_result(query_result)\n",
      "File \u001b[1;32mc:\\Users\\ericy\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\llama_index\\core\\vector_stores\\simple.py:376\u001b[0m, in \u001b[0;36mSimpleVectorStore.query\u001b[1;34m(self, query, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m     top_similarities, top_ids \u001b[38;5;241m=\u001b[39m get_top_k_mmr_embeddings(\n\u001b[0;32m    369\u001b[0m         query_embedding,\n\u001b[0;32m    370\u001b[0m         embeddings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    373\u001b[0m         mmr_threshold\u001b[38;5;241m=\u001b[39mmmr_threshold,\n\u001b[0;32m    374\u001b[0m     )\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m VectorStoreQueryMode\u001b[38;5;241m.\u001b[39mDEFAULT:\n\u001b[1;32m--> 376\u001b[0m     top_similarities, top_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_top_k_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[43msimilarity_top_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_top_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid query mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;241m.\u001b[39mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ericy\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\llama_index\\core\\indices\\query\\embedding_utils.py:30\u001b[0m, in \u001b[0;36mget_top_k_embeddings\u001b[1;34m(query_embedding, embeddings, similarity_fn, similarity_top_k, embedding_ids, similarity_cutoff)\u001b[0m\n\u001b[0;32m     28\u001b[0m similarity_heap: List[Tuple[\u001b[38;5;28mfloat\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, emb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(embeddings_np):\n\u001b[1;32m---> 30\u001b[0m     similarity \u001b[38;5;241m=\u001b[39m \u001b[43msimilarity_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_embedding_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m similarity_cutoff \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m similarity \u001b[38;5;241m>\u001b[39m similarity_cutoff:\n\u001b[0;32m     32\u001b[0m         heapq\u001b[38;5;241m.\u001b[39mheappush(similarity_heap, (similarity, embedding_ids[i]))\n",
      "File \u001b[1;32mc:\\Users\\ericy\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\llama_index\\core\\base\\embeddings\\base.py:62\u001b[0m, in \u001b[0;36msimilarity\u001b[1;34m(embedding1, embedding2, mode)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mdot(embedding1, embedding2)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 62\u001b[0m     product \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m     norm \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(embedding1) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(embedding2)\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m product \u001b[38;5;241m/\u001b[39m norm\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./Wizard-Vicuna-30B-Uncensored-GPTQ\", device_map='cuda')\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./Wizard-Vicuna-30B-Uncensored-GPTQ\", device_map='cuda')\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "system = \"You are a professor of psychology who eagerly answers the user's questions and backs up claims using citations and context given. You provide long, detailed and eloquent explanations that are easy to understand\"\n",
    "input_text = \"SYSTEM: \" + system\n",
    "in_length = 0\n",
    "\n",
    "def inference(input_text):\n",
    "    global in_length\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "    in_length = len(input_text)\n",
    "    outputs = model.generate(**inputs, max_length = 4096)\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "def output(generated_text):\n",
    "    global in_length\n",
    "    print(\"ASSISTANT: \" + generated_text[in_length+1:])\n",
    "    print(\"\\n\")\n",
    "\n",
    "def get_query(query):\n",
    "    response = query_engine.query(query)\n",
    "    context = \"Context:\\n\"\n",
    "    for i in range(top_k):\n",
    "        context = context + response.source_nodes[i].text + \"\\n\\n\"\n",
    "    return(context)\n",
    "\n",
    "\n",
    "print(\"\\n\" * 50)    \n",
    "\n",
    "while True:\n",
    "    user = input(\"USER: \")\n",
    "    context = get_query(user)\n",
    "    print(\"\\n\")\n",
    "    input_text += \" USER:\" + context + \"\\n\" + user + \" ASSISTANT: \"\n",
    "    generated_text = inference(input_text)\n",
    "    output(generated_text)\n",
    "    input_text = generated_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
